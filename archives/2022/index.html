<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Hexo</title><meta name="author" content="ğŸ˜Š"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 6.2.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Hexo</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/#Publications"> Publications</a></li><li class="menus_item"><a class="site-page" href="/"> About</a></li><li class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://phower.me"> Blog</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>ğŸ˜Š</h3><p class="author-bio">â•°(*Â°â–½Â°*)â•¯</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weibo" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weixin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-rss" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="/" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="/" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li></ul></div><a class="cv-links" href="/attaches/CV.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><article><p>Spark-Standalone-HAæ¨¡å¼<br>Spark Standaloneé›†ç¾¤æ˜¯Master-Slavesæ¶æ„çš„é›†ç¾¤æ¨¡å¼,å’Œå¤§éƒ¨åˆ†çš„Master-Slavesç»“æ„é›†ç¾¤ä¸€æ ·,å­˜åœ¨<br>ç€Master å•ç‚¹æ•…éšœ(SPOF)çš„é—®é¢˜ã€‚ç®€å•ç†è§£ä¸ºï¼Œspark-Standalone æ¨¡å¼ä¸‹ä¸º master èŠ‚ç‚¹æ§åˆ¶å…¶ä»–èŠ‚<br>ç‚¹ï¼Œå½“ master èŠ‚ç‚¹å‡ºç°æ•…éšœæ—¶ï¼Œé›†ç¾¤å°±ä¸å¯ç”¨äº†ã€‚ spark-Standalone-HA æ¨¡å¼ä¸‹<br>master èŠ‚ç‚¹ä¸å›ºå®šï¼Œå½“ä¸€ä¸ªå®•æœºæ—¶ï¼Œç«‹å³æ¢å¦ä¸€å°ä¸º master ä¿éšœä¸å‡ºç°æ•…éšœã€‚<br>æ­¤å¤„å› ä¸ºå…ˆå‰é…ç½®æ—¶çš„ zookeeper ç‰ˆæœ¬å’Œ spark ç‰ˆæœ¬ä¸å¤ªå…¼å®¹ï¼Œå¯¼è‡´æ­¤æ¨¡å¼æœ‰æ•…éšœï¼Œéœ€è¦é‡æ–°ä¸‹<br>è½½é…ç½®æ–°çš„ç‰ˆæœ¬çš„ zookeeper<br>é…ç½®ä¹‹å‰éœ€è¦åˆ é™¤ä¸‰å°ä¸»æœºçš„ æ—§ç‰ˆ zookeeper ä»¥åŠ å¯¹åº”çš„è½¯è¿æ¥<br>åœ¨ master èŠ‚ç‚¹ä¸Šé‡æ–°è¿›è¡Œå‰é¢é…ç½®çš„ zookeeper æ“ä½œ</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   1.ä¸Šä¼ apache-zookeeper-3.7.0-bin.tar.gz åˆ°&#x2F;export&#x2F;server&#x2F;ç›®å½•ä¸‹ å¹¶è§£å‹æ–‡ä»¶ 2.åœ¨ &#x2F;export&#x2F;server ç›®å½•ä¸‹åˆ›å»ºè½¯è¿æ¥ 3.è¿›å…¥ &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; å°† zoo_sample.cfg æ–‡ä»¶å¤åˆ¶ä¸ºæ–°æ–‡ä»¶ zoo.cfg 4.æ¥ä¸Šæ­¥ç»™ zoo.cfg æ·»åŠ å†…å®¹ 5.è¿›å…¥ &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas ç›®å½•åœ¨æ­¤ç›®å½•ä¸‹åˆ›å»º myid æ–‡ä»¶ï¼Œå°† 1 å†™å…¥è¿› å»6.å°† master èŠ‚ç‚¹ä¸­ &#x2F;export&#x2F;server&#x2F;zookeeper-3.7.0 è·¯å¾„ä¸‹å†…å®¹æ¨é€ç»™slave1 å’Œ slave2 7.æ¨é€æˆåŠŸåï¼Œåˆ†åˆ«åœ¨ slave1 å’Œ slave2 ä¸Šåˆ›å»ºè½¯è¿æ¥ 8.æ¥ä¸Šæ­¥æ¨é€å®Œæˆåå°† slave1 å’Œ slave2 çš„ &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;æ–‡ä»¶å¤¹ ä¸‹çš„ myid ä¸­çš„å†…å®¹åˆ†åˆ«æ”¹ä¸º 2 å’Œ 3 é…ç½®ç¯å¢ƒå˜é‡ï¼š å› å…ˆå‰é…ç½® zookeeper æ—¶å€™åˆ›å»ºè¿‡è½¯è¿æ¥ä¸”ä»¥ â€™zookeeperâ€˜ ä¸ºè·¯å¾„ï¼Œæ‰€ä»¥ä¸ç”¨é…ç½®ç¯å¢ƒå˜é‡ï¼Œæ­¤ å¤„ä¹Ÿæ˜¯åˆ›å»ºè½¯è¿æ¥çš„æ–¹ä¾¿ä¹‹å¤„</p>
</blockquote>
</blockquote>
<p>è¿›å…¥ &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf æ–‡ä»¶å¤¹ ä¿®æ”¹ spark-env.sh æ–‡ä»¶å†…å®¹</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf<br>    vim spark-env.sh</p>
</blockquote>
</blockquote>
<p>ä¸º 83 è¡Œå†…å®¹åŠ ä¸Šæ³¨é‡Šï¼Œæ­¤éƒ¨åˆ†åŸä¸ºæŒ‡å®š æŸå°ä¸»æœº åš master ï¼ŒåŠ ä¸Šæ³¨é‡Šåå³ä¸º ä»»ä½•ä¸»æœºéƒ½<br>å¯ä»¥åš master</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   ç»“æœæ˜¾ç¤ºï¼š<br>     â€¦â€¦<br>      82 # å‘ŠçŸ¥Sparkçš„masterè¿è¡Œåœ¨å“ªä¸ªæœºå™¨ä¸Š 83 # export SPARK_MASTER_HOST&#x3D;master<br>     â€¦â€¦â€¦</p>
</blockquote>
</blockquote>
<p>æ–‡æœ«æ·»åŠ å†…å®¹</p>
<blockquote>
<blockquote>
<blockquote>
<p>SPARK_DAEMON_JAVA_OPTS&#x3D;â€-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER -<br>       Dspark.deploy.zookeeper.url&#x3D;master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark-haâ€<br>        #spark.deploy.recoveryMode<br>        æŒ‡å®šHAæ¨¡å¼ åŸºäºZookeeperå®ç°<br>        #æŒ‡å®šZookeeperçš„è¿æ¥åœ°å€<br>        #æŒ‡å®šåœ¨Zookeeperä¸­æ³¨å†Œä¸´æ—¶èŠ‚ç‚¹çš„è·¯å¾„</p>
</blockquote>
</blockquote>
</blockquote>
<p>åˆ†å‘ spark-env.sh åˆ° salve1 å’Œ slave2 ä¸Š</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<pre><code>scp spark-env.sh slave1:/export/server/spark/conf/ 
scp spark-env.sh slave2:/export/server/spark/conf/
</code></pre>
</blockquote>
</blockquote>
<p>å¯åŠ¨ä¹‹å‰ç¡®ä¿ Zookeeper å’Œ HDFS å‡å·²ç»å¯åŠ¨<br>å¯åŠ¨é›†ç¾¤:</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   #åœ¨ master ä¸Š å¯åŠ¨ä¸€ä¸ªmaster å’Œå…¨éƒ¨worker &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-all.sh # æ³¨æ„, ä¸‹é¢å‘½ä»¤åœ¨ slave1 ä¸Šæ‰§è¡Œ å¯åŠ¨ slave1 ä¸Šçš„ master åšå¤‡ç”¨ master &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-master.sh<br>    ç»“æœæ˜¾ç¤ºï¼š<br>    (base) [root@master ~]# jps<br>    37328 DataNode<br>    41589 Master<br>    35798 QuorumPeerMain<br>    38521 ResourceManager<br>    46281 Jps<br>    38907 NodeManager<br>    41821 Worker<br>    36958 NameNode (base)<br>    [root@slave1 sbin]# jps<br>    36631 DataNode<br>    48135 Master<br>    35385 QuorumPeerMain<br>    37961 NodeManager<br>    40970 Worker<br>    48282 Jps<br>    37276 SecondaryNameNode</p>
</blockquote>
</blockquote>
<p>è®¿é—® WebUI ç•Œé¢</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<pre><code>http://master:8081/
</code></pre>
<p>   <a target="_blank" rel="noopener" href="http://slave1:8082/">http://slave1:8082/</a></p>
</blockquote>
</blockquote>
<p>æ­¤æ—¶ kill æ‰ master ä¸Šçš„ master å‡è®¾ master ä¸»æœºå®•æœºæ‰</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   #masterä¸»æœº master çš„è¿›ç¨‹å· kill -9 41589 ç»“æœæ˜¾ç¤ºï¼š (base) [root@master ~]# jps 37328 DataNode 90336 Jps 35798 QuorumPeerMain 38521 ResourceManager 38907 NodeManager 41821 Worker 36958 NameNode</p>
</blockquote>
</blockquote>
<p>è®¿é—® slave1 çš„ WebUI</p>
<blockquote>
<blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://slave1:8082/">http://slave1:8082/</a></p>
</blockquote>
</blockquote>
</blockquote>
<p>è¿›è¡Œä¸»å¤‡åˆ‡æ¢çš„æµ‹è¯•<br>æäº¤ä¸€ä¸ª spark ä»»åŠ¡åˆ°å½“å‰ æ´»è·ƒçš„ masterä¸Š :</p>
<blockquote>
<blockquote>
<blockquote>
<p>&#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit â€“master spark:&#x2F;&#x2F;master:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000</p>
</blockquote>
</blockquote>
</blockquote>
<p>å¤åˆ¶æ ‡ç­¾ kill æ‰ master çš„ è¿›ç¨‹å·<br>å†æ¬¡è®¿é—® master çš„ WebUI</p>
<blockquote>
<blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://master:8081/">http://master:8081/</a><br>      ç½‘é¡µè®¿é—®ä¸äº†ï¼</p>
</blockquote>
</blockquote>
</blockquote>
<p>å†æ¬¡è®¿é—® slave1 çš„ WebUI</p>
<blockquote>
<blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://slave1:8082/">http://slave1:8082/</a></p>
</blockquote>
</blockquote>
</blockquote>
<p>å¯ä»¥çœ‹åˆ°å½“å‰æ´»è·ƒçš„ master æç¤ºä¿¡æ¯</p>
<blockquote>
<blockquote>
<blockquote>
<p>(base) [root@master ~]# &#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit â€“master spark:&#x2F;&#x2F;master:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000 22&#x2F;03&#x2F;29 16:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platformâ€¦ using builtin-java classes where applicable 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnectâ€¦ 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnectionâ€¦ 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnectâ€¦ Pi is roughly 3.140960 (base) [root@master ~]#</p>
</blockquote>
</blockquote>
</blockquote>
<p>Spark On YARNæ¨¡å¼</p>
<blockquote>
<blockquote>
<blockquote>
<p>åœ¨å·²æœ‰YARNé›†ç¾¤çš„å‰æä¸‹åœ¨å•ç‹¬å‡†å¤‡Spark StandAloneé›†ç¾¤,å¯¹èµ„æºçš„åˆ©ç”¨å°±ä¸é«˜.Spark On YARN, æ— </p>
</blockquote>
</blockquote>
</blockquote>
<p>éœ€éƒ¨ç½²Sparké›†ç¾¤, åªè¦æ‰¾ä¸€å°æœåŠ¡å™¨, å……å½“Sparkçš„å®¢æˆ·ç«¯<br>ä¿è¯ HADOOP_CONF_å’Œ DIR_YARN_CONF_DIR å·²ç»é…ç½®åœ¨ spark-env.sh å’Œç¯å¢ƒå˜é‡ä¸­ ï¼ˆæ³¨: å‰é¢é…ç½®spark-Standlone æ—¶å·²ç»é…ç½®è¿‡æ­¤é¡¹äº†ï¼‰</p>
<blockquote>
<blockquote>
<blockquote>
<p>spark-env.sh æ–‡ä»¶éƒ¨åˆ†æ˜¾ç¤ºï¼š â€¦. 77 ## HADOOPè½¯ä»¶é…ç½®æ–‡ä»¶ç›®å½•ï¼Œè¯»å–HDFSä¸Šæ–‡ä»¶å’Œè¿è¡ŒYARNé›†ç¾¤ 78 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop 79 YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop â€¦.</p>
</blockquote>
</blockquote>
</blockquote>
<p>é“¾æ¥åˆ° YARN ä¸­ï¼ˆæ³¨: äº¤äº’å¼ç¯å¢ƒ pyspark å’Œ spark-shell æ— æ³•è¿è¡Œ clusteræ¨¡å¼ï¼‰<br>bin&#x2F;pyspark â€“master yarn â€“deploy-mode client|cluster # â€“deploy-mode é€‰é¡¹æ˜¯æŒ‡å®šéƒ¨ç½²æ¨¡å¼, é»˜è®¤æ˜¯ å®¢æˆ·ç«¯æ¨¡å¼ # clientå°±æ˜¯å®¢æˆ·ç«¯æ¨¡å¼ # clusterå°±æ˜¯é›†ç¾¤æ¨¡å¼ # â€“deploy-mode ä»…å¯ä»¥ç”¨åœ¨YARNæ¨¡å¼ä¸‹<br> bin&#x2F;spark-shell â€“master yarn â€“deploy-mode client|cluster<br>bin&#x2F;spark-submit â€“master yarn â€“deploy-mode client|cluster &#x2F;xxx&#x2F;xxx&#x2F;xxx.py å‚æ•°</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>  spark-submit å’Œ spark-shell å’Œ pysparkçš„ç›¸å…³å‚æ•°</p>
</blockquote>
</blockquote>
<ul>
<li>bin&#x2F;pyspark: pysparkè§£é‡Šå™¨sparkç¯å¢ƒ - bin&#x2F;spark-shell: scalaè§£é‡Šå™¨sparkç¯å¢ƒ - bin&#x2F;spark-submit: æäº¤jaråŒ…æˆ–Pythonæ–‡ä»¶æ‰§è¡Œçš„å·¥å…· - bin&#x2F;spark-sql: sparksqlå®¢æˆ·ç«¯å·¥å…·<br>  è¿™4ä¸ªå®¢æˆ·ç«¯å·¥å…·çš„å‚æ•°åŸºæœ¬é€šç”¨.ä»¥spark-submit ä¸ºä¾‹: bin&#x2F;spark-submit â€“master spark:&#x2F;&#x2F;master:7077 xxx.py&#96;<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>  Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]<br>   Usage: spark-submit â€“kill [submission ID] â€“master [spark:&#x2F;&#x2F;â€¦]<br>   Usage: spark-submit â€“status [submission ID] â€“master [spark:&#x2F;&#x2F;â€¦]<br>   Usage: spark-submit run-example [options] example-class [example args] </p>
<blockquote>
</blockquote>
<p>   Options: â€“master MASTER_URL spark:&#x2F;&#x2F;host:port, mesos:&#x2F;&#x2F;host:port, yarn, k8s:&#x2F;&#x2F;<a href="https://host:port">https://host:port</a>, or </p>
<blockquote>
</blockquote>
<p>   local (Default: local[*]). â€“deploy-mode DEPLOY_MODE éƒ¨ç½²æ¨¡å¼ client æˆ–è€… cluster é»˜è®¤æ˜¯client â€“class CLASS_NAME è¿è¡Œjavaæˆ–è€…scala class(for Java &#x2F; Scala apps). â€“name NAME ç¨‹åºçš„åå­— â€“jars JARS Comma-separated list of jars to include on the </p>
<blockquote>
</blockquote>
<p>   driver and executor classpaths. â€“packages Comma-separated list of maven coordinates of </p>
<blockquote>
</blockquote>
<p>   jars to include on the driver and executor classpaths. Will </p>
<blockquote>
</blockquote>
<p>   search the local maven repo, then maven central and any </p>
<blockquote>
</blockquote>
<p>   additional remote repositories given by â€“repositories. The </p>
<blockquote>
</blockquote>
<p>   format for the coordinates should be </p>
<blockquote>
</blockquote>
<p>   groupId:artifactId:version.<br>   â€“exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in<br>â€“ packages to avoid dependency conflicts. â€“repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with<br> â€“ packages.<br> â€“py-files PY_FILES æŒ‡å®šPythonç¨‹åºä¾èµ–çš„å…¶å®ƒpythonæ–‡ä»¶<br> â€“files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName).<br>  â€“archives ARCHIVES Comma-separated list of archives to be extracted into the working directory of each executor.<br> â€“conf,<br> -c PROP&#x3D;VALUE æ‰‹åŠ¨æŒ‡å®šé…ç½®<br> â€“properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf&#x2F;spark- defaults.conf. â€“driver-memory MEM Driverçš„å¯ç”¨å†…å­˜(Default: 1024M). â€“driver-java-options Driverçš„ä¸€äº›Javaé€‰é¡¹ â€“driver-library-path Extra library path entries to pass to the driver. â€“driver-class-path Extra class path entries to pass to the driver. Note that jars added with â€“jars are automatically included in the classpath.<br> â€“executor-memory MEM Executorçš„å†…å­˜ (Default: 1G).<br> â€“proxy-user NAME User to impersonate when submitting the application. This argument does not work with<br> â€“principal &#x2F;<br> â€“keytab.<br> â€“help,<br> -h æ˜¾ç¤ºå¸®åŠ©æ–‡ä»¶<br>  â€“verbose,<br>  -v Print additional debug output. â€“version, æ‰“å°ç‰ˆæœ¬ Cluster deploy mode only(é›†ç¾¤æ¨¡å¼ä¸“å±):<br>   â€“driver-cores NUM Driverå¯ç”¨çš„çš„CPUæ ¸æ•°(Default: 1). Spark standalone or Mesos with cluster deploy mode only:<br>   â€“supervise å¦‚æœç»™å®š, å¯ä»¥å°è¯•é‡å¯Driver Spark standalone, Mesos or K8s with cluster deploy mode only:<br>   â€“kill SUBMISSION_ID æŒ‡å®šç¨‹åºID kill â€“status SUBMISSION_ID æŒ‡å®šç¨‹åºID æŸ¥çœ‹è¿è¡ŒçŠ¶æ€ Spark standalone, Mesos and Kubernetes only:<br>   â€“total-executor-cores NUM æ•´ä¸ªä»»åŠ¡å¯ä»¥ç»™Executorå¤šå°‘ä¸ªCPUæ ¸å¿ƒç”¨ Spark standalone, YARN and Kubernetes only:<br>    â€“executor-cores NUM å•ä¸ªExecutorèƒ½ä½¿ç”¨å¤šå°‘CPUæ ¸å¿ƒ Spark on YARN and Kubernetes only(YARNæ¨¡å¼ä¸‹):<br>    â€“num-executors NUM Executoråº”è¯¥å¼€å¯å‡ ä¸ª<br>    â€“principal PRINCIPAL Principal to be used to login to KDC.<br>    â€“keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. Spark on YARN only:<br>    â€“queue QUEUE_NAME æŒ‡å®šè¿è¡Œçš„YARNé˜Ÿåˆ—(Default: â€œdefaultâ€)</p>
</blockquote>
</blockquote>
</li>
</ul>
<p>å¯åŠ¨ YARN çš„å†å²æœåŠ¡å™¨</p>
<blockquote>
<blockquote>
<blockquote>
<p>cd &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;sbin .&#x2F;mr-jobhistory-daemon.sh start historyserver</p>
</blockquote>
</blockquote>
</blockquote>
<p>è®¿é—®WebUIç•Œé¢</p>
<blockquote>
<blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://master:19888/">http://master:19888/</a></p>
</blockquote>
</blockquote>
</blockquote>
<p>client æ¨¡å¼æµ‹è¯•</p>
<blockquote>
<blockquote>
<blockquote>
<p>SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark ${SPARK_HOME}&#x2F;bin&#x2F;spark-submit â€“master yarn â€“deploy-mode client â€“ driver-memory 512m â€“executor-memory 512m â€“num-executors 1 â€“total- executor-cores 2 ${SPARK_HOME}&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py</p>
</blockquote>
</blockquote>
</blockquote>
<p>cluster æ¨¡å¼æµ‹è¯•</p>
<blockquote>
<blockquote>
<blockquote>
<p>SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark ${SPARK_HOME}&#x2F;bin&#x2F;spark-submit â€“master yarn â€“deploy-mode cluster â€“driver- memory 512m â€“executor-memory 512m â€“num-executors 1 â€“total-executor-cores 2 â€“conf â€œspark.pyspark.driver.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3â€ â€“conf â€œspark.pyspark.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3â€ ${SPARK_HOME}&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 3**</p>
</blockquote>
</blockquote>
</blockquote>
</details>




<hr>
<p>title: â€˜ã€ŠSpark local&amp; stand-aloneé…ç½®ã€‹â€™<br>toc: false<br>comments: true<br>keywords: â€˜â€™<br>description: â€˜â€™<br>date: 2022-05-22 16:18:57<br>updated: 2022-05-22 16:18:57<br>categories:<br>tags:<br>top:<br>academia: true</p>
<hr>
<h1 id="ã€ŠSpark-local-amp-stand-aloneé…ç½®ã€‹"><a href="#ã€ŠSpark-local-amp-stand-aloneé…ç½®ã€‹" class="headerlink" title="ã€ŠSpark local&amp; stand-aloneé…ç½®ã€‹"></a>ã€ŠSpark local&amp; stand-aloneé…ç½®ã€‹</h1><details>
<summary>é˜…è¯»å…¨æ–‡</summary>

<p>**<summary>æœ¬åœ°æ¨¡å¼(å•æœº) æœ¬åœ°æ¨¡å¼å°±æ˜¯ä»¥ä¸€ä¸ªç‹¬ç«‹çš„è¿›ç¨‹,é€šè¿‡å…¶å†…éƒ¨çš„å¤šä¸ªçº¿ç¨‹æ¥æ¨¡æ‹Ÿæ•´ä¸ªSparkè¿è¡Œæ—¶ç¯å¢ƒ<br>Anaconda On Linux å®‰è£… (å•å°æœåŠ¡å™¨è„šæœ¬å®‰è£…)<br>å®‰è£…ä¸Šä¼ å®‰è£…åŒ…: èµ„æ–™ä¸­æä¾›çš„Anaconda3-2021.05-Linux-x86_64.shæ–‡ä»¶åˆ°LinuxæœåŠ¡å™¨ä¸Šå®‰è£…<br>ä½ç½®åœ¨ &#x2F;export&#x2F;server:</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server<br>   #è¿è¡Œæ–‡ä»¶ sh Anaconda3-2021.05-Linux-x86_64.sh<br>   è¿‡ç¨‹æ˜¾ç¤ºï¼š<br>   â€¦<br>   #å‡ºç°å†…å®¹é€‰ yes Please answer â€˜yesâ€™ or â€˜noâ€™:â€™ &gt;&gt;&gt; yes â€¦<br>   #å‡ºç°æ·»åŠ è·¯å¾„ï¼š&#x2F;export&#x2F;server&#x2F;anaconda3<br>   â€¦<br>   [&#x2F;root&#x2F;anaconda3] </p>
<blockquote>
</blockquote>
<p>   &#x2F;export&#x2F;server&#x2F;anaconda3 PREFIX&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3<br>   â€¦</p>
</blockquote>
</blockquote>
<p>å®‰è£…å®Œæˆå, é€€å‡ºç»ˆç«¯ï¼Œ é‡æ–°è¿›æ¥:</p>
<blockquote>
<blockquote>
<blockquote>
<p>exit<br>   ç»“æœæ˜¾ç¤ºï¼š<br>   #çœ‹åˆ°è¿™ä¸ªBaseå¼€å¤´è¡¨æ˜å®‰è£…å¥½äº†.baseæ˜¯é»˜è®¤çš„è™šæ‹Ÿç¯å¢ƒ. Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1 (base)<br>   [root@node1 ~]#</p>
</blockquote>
</blockquote>
</blockquote>
<p>åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ pyspark åŸºäº python3.8</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   conda create -n pyspark python&#x3D;3.8</p>
</blockquote>
</blockquote>
<p>åˆ‡æ¢åˆ°è™šæ‹Ÿç¯å¢ƒå†…</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   conda activate pyspark ç»“æœæ˜¾ç¤ºï¼š (base) [root@node1 ~]# conda activate pyspark (pyspark) [root@node1 ~]#</p>
</blockquote>
</blockquote>
<p>åœ¨è™šæ‹Ÿç¯å¢ƒå†…å®‰è£…åŒ… ï¼ˆæœ‰WARNINGä¸ç”¨ç®¡ï¼‰</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   pip install pyhive pyspark jieba -i <a target="_blank" rel="noopener" href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></p>
</blockquote>
</blockquote>
<p>spark å®‰è£…<br>å°†æ–‡ä»¶ä¸Šä¼ åˆ° &#x2F;export&#x2F;server é‡Œé¢ ï¼Œè§£å‹</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server # è§£å‹ tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C &#x2F;export&#x2F;server&#x2F;</p>
</blockquote>
</blockquote>
<p>å»ºç«‹è½¯è¿æ¥</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark</p>
</blockquote>
</blockquote>
<p>æ·»åŠ ç¯å¢ƒå˜é‡</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   SPARK_HOME: è¡¨ç¤ºSparkå®‰è£…è·¯å¾„åœ¨å“ªé‡Œ<br>   PYSPARK_PYTHON: è¡¨ç¤ºSparkæƒ³è¿è¡ŒPythonç¨‹åº, é‚£ä¹ˆå»å“ªé‡Œæ‰¾pythonæ‰§è¡Œå™¨<br>   JAVA_HOME: å‘ŠçŸ¥Spark Javaåœ¨å“ªé‡Œ<br>   HADOOP_CONF_DIR: å‘ŠçŸ¥Spark Hadoopçš„é…ç½®æ–‡ä»¶åœ¨å“ªé‡Œ<br>   HADOOP_HOME: å‘ŠçŸ¥Spark Hadoopå®‰è£…åœ¨å“ªé‡Œ</p>
</blockquote>
</blockquote>
<p>   vim &#x2F;etc&#x2F;profile<br>   å†…å®¹ï¼š<br>   â€¦..<br>   æ³¨ï¼šæ­¤éƒ¨åˆ†ä¹‹å‰é…ç½®è¿‡ï¼Œæ­¤éƒ¨åˆ†ä¸éœ€è¦åœ¨é…ç½®<br>   #JAVA_HOME export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar </p>
<p>   #HADOOP_HOME export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin </p>
<p>   #ZOOKEEPER_HOME export ZOOKEEPER_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper export PATH&#x3D;$PATH:$ZOOKEEPER_HOME&#x2F;bin â€¦.. </p>
<p>   #å°†ä»¥ä¸‹éƒ¨åˆ†æ·»åŠ è¿›å» #SPARK_HOME export SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark #HADOOP_CONF_DIR export HADOOP_CONF_DIR&#x3D;$HADOOP_HOME&#x2F;etc&#x2F;hadoop #PYSPARK_PYTHON export PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python<br>   vim .bashrc<br>   å†…å®¹æ·»åŠ è¿›å»ï¼š </p>
<p>   #JAVA_HOME<br>   export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241<br>   #PYSPARK_PYTHON<br>   export PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python</p>
<p>é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   source &#x2F;etc&#x2F;profile<br>   source ~&#x2F;.bashrc</p>
</blockquote>
</blockquote>
<p>è¿›å…¥ &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F; æ–‡ä»¶å¤¹</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;</p>
</blockquote>
</blockquote>
<p>å¼€å¯ </p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>  .&#x2F;pyspark ç»“æœæ˜¾ç¤ºï¼š (base) [root@master bin]# .&#x2F;pyspark<br>   Python 3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0] :: Anaconda, Inc. on linux<br>   Type â€œhelpâ€, â€œcopyrightâ€, â€œcreditsâ€ or â€œlicenseâ€ for more information.<br>   Setting default log level to â€œWARNâ€. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 2022-03-15 20:37:04,612 WARN util.NativeCodeLoader: Unable to load native- hadoop library for your platformâ€¦ using builtin-java classes where applicable<br>    Welcome to<br>         __              __<br>    __ &#x2F; <strong>&#x2F;</strong> ___ _<em><em><strong>&#x2F; &#x2F;</strong><br>     <em>\ / _ / _ &#96;&#x2F; _<em>&#x2F; â€˜</em>&#x2F;<br>     &#x2F;_</em> &#x2F; .</em><em>&#x2F;_,</em>&#x2F;</em>&#x2F; &#x2F;<em>&#x2F;_\ version 3.2.0<br>        &#x2F;</em>&#x2F;<br>    Using Python version 3.8.12 (default, Oct 12 2021 13:49:34) Spark context Web UI available at <a target="_blank" rel="noopener" href="http://master:4040/">http://master:4040</a> Spark context available as â€˜scâ€™ (master &#x3D; local[*], app id &#x3D; local- 1647347826262). SparkSession available as â€˜sparkâ€™. &gt;&gt;&gt;</p>
</blockquote>
</blockquote>
<p>æŸ¥çœ‹WebUIç•Œé¢</p>
<blockquote>
<blockquote>
<blockquote>
<p>æµè§ˆå™¨è®¿é—®ï¼š<br>    <a target="_blank" rel="noopener" href="http://node1:4040/">http://node1:4040/</a></p>
</blockquote>
</blockquote>
</blockquote>
<p>é€€å‡º</p>
<blockquote>
<blockquote>
<blockquote>
<p>conda deactivate</p>
</blockquote>
</blockquote>
</blockquote>
<p>Standaloneæ¨¡å¼(é›†ç¾¤) Sparkä¸­çš„å„ä¸ªè§’è‰²ä»¥ç‹¬ç«‹è¿›ç¨‹çš„å½¢å¼å­˜åœ¨,å¹¶ç»„æˆSparké›†ç¾¤ç¯å¢ƒ Anaconda On Linux å®‰è£… (å•å°æœåŠ¡å™¨è„šæœ¬å®‰è£… æ³¨ï¼šåœ¨ slave1 å’Œ slave2 ä¸Šéƒ¨ç½²)<br>å®‰è£…ä¸Šä¼ å®‰è£…åŒ…: èµ„æ–™ä¸­æä¾›çš„Anaconda3-2021.05-Linux-x86_64.shæ–‡ä»¶åˆ°LinuxæœåŠ¡å™¨ä¸Šå®‰è£…ä½ç½®åœ¨ &#x2F;export&#x2F;server:</p>
<p>cd &#x2F;export&#x2F;server # è¿è¡Œæ–‡ä»¶ sh Anaconda3-2021.05-Linux-x86_64.sh</p>
<blockquote>
<blockquote>
<blockquote>
<p>è¿‡ç¨‹æ˜¾ç¤ºï¼š<br> â€¦<br> #å‡ºç°å†…å®¹é€‰ yes<br>  Please answer â€˜yesâ€™ or â€˜noâ€™:â€™<br>yes<br>   â€¦<br>   #å‡ºç°æ·»åŠ è·¯å¾„ï¼š&#x2F;export&#x2F;server&#x2F;anaconda3<br>   â€¦<br>   [&#x2F;root&#x2F;anaconda3] &gt;&gt;&gt; &#x2F;export&#x2F;server&#x2F;anaconda3 PREFIX&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3<br>   â€¦</p>
</blockquote>
</blockquote>
</blockquote>
<p>å®‰è£…å®Œæˆå, é€€å‡ºç»ˆç«¯ï¼Œ</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>  é‡æ–°è¿›æ¥:<br>  exit<br>  ç»“æœæ˜¾ç¤ºï¼š<br>   #çœ‹åˆ°è¿™ä¸ªBaseå¼€å¤´è¡¨æ˜å®‰è£…å¥½äº†.baseæ˜¯é»˜è®¤çš„è™šæ‹Ÿç¯å¢ƒ.<br>    Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1<br>   â€¦</p>
</blockquote>
</blockquote>
<p>åœ¨ master èŠ‚ç‚¹ä¸ŠæŠŠ .&#x2F;bashrc å’Œ profile åˆ†å‘ç»™ slave1 å’Œ slave2</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   #åˆ†å‘ .bashrc : scp <del>&#x2F;.bashrc root@slave1:</del>&#x2F; scp <del>&#x2F;.bashrc root@slave2:</del>&#x2F; #åˆ†å‘ profile : scp &#x2F;etc&#x2F;profile&#x2F; root@slave1:&#x2F;etc&#x2F; scp &#x2F;etc&#x2F;profile&#x2F; root@slave2:&#x2F;etc&#x2F;<br>   â€¦</p>
</blockquote>
</blockquote>
<p>åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ pyspark åŸºäº python3.8</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   conda create -n pyspark python&#x3D;3.8</p>
</blockquote>
</blockquote>
<p>åˆ‡æ¢åˆ°è™šæ‹Ÿç¯å¢ƒå†…</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   conda activate pyspark ç»“æœæ˜¾ç¤ºï¼š (base) [root@node1 ~]# conda activate pyspark (pyspark)<br>åœ¨è™šæ‹Ÿç¯å¢ƒå†…å®‰è£…åŒ… ï¼ˆæœ‰WARNINGä¸ç”¨ç®¡ï¼‰<br>    pip install pyhive pyspark jieba -i <a target="_blank" rel="noopener" href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a><br>    spark å®‰è£…<br>å°†æ–‡ä»¶ä¸Šä¼ åˆ° &#x2F;export&#x2F;server é‡Œé¢ ï¼Œè§£å‹</p>
</blockquote>
</blockquote>
<p>master èŠ‚ç‚¹èŠ‚ç‚¹è¿›å…¥ &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf ä¿®æ”¹ä»¥ä¸‹é…ç½®æ–‡ä»¶</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf</p>
</blockquote>
</blockquote>
<p>å°†æ–‡ä»¶ workers.template æ”¹åä¸º workersï¼Œå¹¶é…ç½®æ–‡ä»¶å†…å®¹</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   mv workers.template workers vim workers<br>    # localhoståˆ é™¤ï¼Œå†…å®¹è¿½åŠ æ–‡æœ«ï¼š node1<br>    node2<br>    node3<br>    # åŠŸèƒ½: è¿™ä¸ªæ–‡ä»¶å°±æ˜¯æŒ‡ç¤ºäº† å½“å‰SparkStandAloneç¯å¢ƒä¸‹, æœ‰å“ªäº›worker</p>
</blockquote>
</blockquote>
<p>å°†æ–‡ä»¶ spark-env.sh.template æ”¹åä¸º spark-env.shï¼Œå¹¶é…ç½®ç›¸å…³å†…å®¹</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   mv spark-env.sh.template spark-env.sh vim spark-env.sh</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>æ–‡æœ«è¿½åŠ å†…å®¹ï¼š<br>   ##è®¾ç½®JAVAå®‰è£…ç›®å½• JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk<br>   ##HADOOPè½¯ä»¶é…ç½®æ–‡ä»¶ç›®å½•ï¼Œè¯»å–HDFSä¸Šæ–‡ä»¶å’Œè¿è¡ŒYARNé›†ç¾¤ HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop ## æŒ‡å®šsparkè€å¤§Masterçš„IPå’Œæäº¤ä»»åŠ¡çš„é€šä¿¡ç«¯å£ # å‘ŠçŸ¥Sparkçš„masterè¿è¡Œåœ¨å“ªä¸ªæœºå™¨ä¸Š export SPARK_MASTER_HOST&#x3D;master<br>    #å‘ŠçŸ¥sparkmasterçš„é€šè®¯ç«¯å£ export SPARK_MASTER_PORT&#x3D;7077<br>    # å‘ŠçŸ¥spark masterçš„ webuiç«¯å£ SPARK_MASTER_WEBUI_PORT&#x3D;8080<br>    # worker cpuå¯ç”¨æ ¸æ•° SPARK_WORKER_CORES&#x3D;1<br>    # workerå¯ç”¨å†…å­˜ SPARK_WORKER_MEMORY&#x3D;1g<br>    # workerçš„å·¥ä½œé€šè®¯åœ°å€ SPARK_WORKER_PORT&#x3D;7078<br>    # workerçš„ webuiåœ°å€ SPARK_WORKER_WEBUI_PORT&#x3D;8081<br>    ## è®¾ç½®å†å²æœåŠ¡å™¨ # é…ç½®çš„æ„æ€æ˜¯ å°†sparkç¨‹åºè¿è¡Œçš„å†å²æ—¥å¿— å­˜åˆ°hdfsçš„&#x2F;sparklogæ–‡ä»¶å¤¹ä¸­ SPARK_HISTORY_OPTS&#x3D;â€- Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;master:8020&#x2F;sparklog&#x2F; - Dspark.history.fs.cleaner.enabled&#x3D;trueâ€</p>
</blockquote>
</blockquote>
</blockquote>
<p>å¼€å¯ hadoop çš„ hdfs å’Œ yarn é›†ç¾¤</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<pre><code>start-dfs.sh 
start-yarn.sh
</code></pre>
<p>åœ¨HDFSä¸Šåˆ›å»ºç¨‹åºè¿è¡Œå†å²è®°å½•å­˜æ”¾çš„æ–‡ä»¶å¤¹ï¼ŒåŒæ · conf æ–‡ä»¶ç›®å½•ä¸‹:</p>
<blockquote>
</blockquote>
<p>   hadoop fs -mkdir &#x2F;sparklog hadoop fs -chmod 777 &#x2F;sparklog</p>
</blockquote>
</blockquote>
<p>å°† spark-defaults.conf.template æ”¹ä¸º spark-defaults.conf å¹¶åšç›¸å…³é…ç½®</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   mv spark-defaults.conf.template spark-defaults.conf vim spark-defaults.conf æ–‡æœ«è¿½åŠ å†…å®¹ä¸ºï¼š # å¼€å¯sparkçš„æ—¥æœŸè®°å½•åŠŸèƒ½ spark.eventLog.enabled true # è®¾ç½®sparkæ—¥å¿—è®°å½•çš„è·¯å¾„ spark.eventLog.dir hdfs:&#x2F;&#x2F;master:8020&#x2F;sparklog&#x2F; # è®¾ç½®sparkæ—¥å¿—æ˜¯å¦å¯åŠ¨å‹ç¼© spark.eventLog.compress true</p>
</blockquote>
</blockquote>
<p>é…ç½® log4j.properties æ–‡ä»¶ å°†æ–‡ä»¶ç¬¬ 19 è¡Œçš„ log4j.rootCategory&#x3D;INFO, console æ”¹ä¸º<br>log4j.rootCategory&#x3D;WARN, console ï¼ˆå³å°†INFO æ”¹ä¸º WARN ç›®çš„ï¼šè¾“å‡ºæ—¥å¿—, è®¾ç½®çº§åˆ«ä¸º<br>WARN åªè¾“å‡ºè­¦å‘Šå’Œé”™è¯¯æ—¥å¿—ï¼ŒINFO åˆ™ä¸ºè¾“å‡ºæ‰€æœ‰ä¿¡æ¯ï¼Œå¤šæ•°ä¸ºæ— ç”¨ä¿¡æ¯ï¼‰</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   mv log4j.properties.template log4j.properties vim log4j.properties ç»“æœæ˜¾ç¤ºï¼š<br>    â€¦<br>    18 # Set everything to be logged to the console<br>    19 log4j.rootCategory&#x3D;WARN, console â€¦.</p>
</blockquote>
</blockquote>
<p>master èŠ‚ç‚¹åˆ†å‘ spark å®‰è£…æ–‡ä»¶å¤¹ åˆ° slave1 å’Œ slave2 ä¸Š</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   master èŠ‚ç‚¹åˆ†å‘ spark å®‰è£…æ–‡ä»¶å¤¹ åˆ° slave1 å’Œ slave2 ä¸Š</p>
</blockquote>
</blockquote>
<p>åœ¨slave1 å’Œ slave2 ä¸Šåšè½¯è¿æ¥</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark</p>
</blockquote>
</blockquote>
<p>é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   source &#x2F;etc&#x2F;profile</p>
</blockquote>
</blockquote>
<p>è¿›å…¥ &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin æ–‡ä»¶ç›®å½•ä¸‹ å¯åŠ¨ start-history-server.sh</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin .&#x2F;start-history-server.sh</p>
</blockquote>
</blockquote>
<p>è®¿é—® WebUI ç•Œé¢</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   æµè§ˆå™¨è®¿é—®ï¼š <a target="_blank" rel="noopener" href="http://master:18080/">http://master:18080/</a></p>
</blockquote>
</blockquote>
</article><article><h1 id="ã€ŠsparkåŸºç¡€ç¯å¢ƒé…ç½®ã€‹"><a href="#ã€ŠsparkåŸºç¡€ç¯å¢ƒé…ç½®ã€‹" class="headerlink" title="ã€ŠsparkåŸºç¡€ç¯å¢ƒé…ç½®ã€‹"></a>ã€ŠsparkåŸºç¡€ç¯å¢ƒé…ç½®ã€‹</h1><p>æ‰“å¼€ä¸€ä¸ªhostsæ˜ å°„æ–‡ä»¶,ä¸ºäº†ä¿è¯åç»­ç›¸äº’å…³è”çš„è™šæ‹Ÿæœºèƒ½å¤Ÿé€šè¿‡ä¸»æœºåè¿›è¡Œè®¿é—®ï¼Œæ ¹æ®å®é™…éœ€æ±‚é…ç½®<br>å¯¹åº”çš„IPå’Œä¸»æœºåæ˜ å°„ï¼Œåˆ†åˆ«å°†ä¸»æœºåmasterã€slave1ã€slave2 ä¸IPåœ°å€ 192.168.88.134ã€<br>192.168.88.135 å’Œ192.168.88.136è¿›è¡Œäº†åŒ¹é…æ˜ å°„(è¿™é‡Œé€šå¸¸è¦æ ¹æ®å®é™…éœ€è¦ï¼Œå°†è¦æ­å»ºçš„é›†ç¾¤ä¸»æœºéƒ½<br>é…ç½®ä¸»æœºåå’ŒIPæ˜ å°„)ã€‚<br>ç¼–è¾‘ &#x2F;etc&#x2F;hosts æ–‡ä»¶</p>
<blockquote>
<blockquote>
<blockquote>
<p>vim &#x2F;etc&#x2F;hosts<br>   å†…å®¹ä¿®æ”¹ä¸ºï¼ˆæ³¨ï¼šä¸‰å°ä¸»æœºå†…å®¹ä¸€æ ·ï¼‰<br>   localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6<br>   192.168.88.135 node1<br>   192.168.88.136 node2<br>   192.168.88.137 node3</p>
</blockquote>
</blockquote>
</blockquote>
<p>ä¸‰ã€é›†ç¾¤é…ç½®æ—¶é—´åŒæ­¥<br>å®šä¹‰ï¼šç½‘ç»œæ—¶é—´æœåŠ¡åè®®ï¼ˆNetwork Time Protocol, NTPï¼‰ï¼Œæ˜¯ç”¨æ¥ä½¿è®¡ç®—æœºæ—¶é—´åŒæ­¥åŒ–çš„ä¸€ç§åè®®ï¼Œå®ƒå¯ä»¥ä½¿è®¡ç®—æœºå¯¹å…¶æœåŠ¡å™¨åšæ—¶é—´åŒæ­¥åŒ–ã€‚<br>åŸå› ï¼šæ—¶é—´åŒæ­¥æœåŠ¡å™¨ï¼Œé¡¾åæ€ä¹‰å°±æ˜¯æ¥åŒæ­¥æ—¶é—´çš„ã€‚åœ¨é›†ç¾¤ä¸­åŒæ­¥æ—¶é—´æœ‰ç€ååˆ†é‡è¦çš„ä½œç”¨ï¼Œè´Ÿè½½å‡è¡¡é›†ç¾¤æˆ–é«˜å¯ç”¨é›†ç¾¤å¦‚æœæ—¶é—´ä¸ä¸€è‡´ï¼Œåœ¨æœåŠ¡å™¨ä¹‹é—´çš„æ•°æ®è¯¯å·®å°±ä¼šå¾ˆå¤§ï¼Œå¯»æ‰¾æ•°æ®ä¾¿ä¼šæˆä¸ºä¸€ä»¶æ£˜æ‰‹çš„äº‹æƒ…ã€‚è‹¥æ˜¯æ—¶é—´æ— æ³•åŒæ­¥ï¼Œé‚£ä¹ˆå°±ç®—æ˜¯å¤‡ä»½äº†æ•°æ®ï¼Œä½ ä¹Ÿå¯èƒ½æ— æ³•åœ¨æ­£ç¡®çš„æ—¶é—´å°†æ­£ç¡®çš„æ•°æ®å¤‡ä»½ã€‚é‚£æŸå¤±å¯å°±å¤§äº†ã€‚<br>yum å®‰è£… ntp ï¼ˆæ³¨ï¼šä¸‰å°ä¸»æœºåšåŒæ ·æ“ä½œï¼‰</p>
<blockquote>
<blockquote>
<blockquote>
<p>yum install ntp -y<br>   å¼€æœºè‡ªå¯åŠ¨ntp<br>   systemctl enable ntpd &amp;&amp; systemctl start ntpd<br>   ç»“æœæ˜¾ç¤ºï¼š [root@master ~]# systemctl enable ntpd &amp;&amp; systemctl start ntpd Created symlink from &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;multi- user.target.wants&#x2F;ntpd.service to &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;ntpd.service.</p>
</blockquote>
</blockquote>
</blockquote>
<p>æˆæƒ 192.168.88.0-192.168.10.255 ç½‘æ®µä¸Šçš„æ‰€æœ‰æœºå™¨å¯ä»¥ä»è¿™å°æœºå™¨ä¸ŠæŸ¥è¯¢å’ŒåŒæ­¥æ—¶é—´</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   #æŸ¥çœ‹ntpé…ç½®æ–‡ä»¶<br>   ls -al &#x2F;etc | grep â€˜ntpâ€™<br>   #æ˜¾ç¤ºå†…å®¹<br>   [root@node1 etc]# ls -al &#x2F;etc | grep â€˜ntpâ€™<br>   drwxr-xr-x 3 root root 52 3æœˆ 10 18:25 ntp<br>   -rw-râ€“râ€“ 1 root root 2041 3æœˆ 10 20:03 ntp.conf<br>    #ç¼–è¾‘å†…å®¹æ·»åŠ  restrict 192.168.88.0 mask 255.255.255.0 ï¼ˆæ³¨ï¼šåœ¨17è¡Œå·¦å³ï¼‰ vim &#x2F;etc&#x2F;ntp.conf<br>    16 # Hosts on local network are less restricted.<br>    17 restrict 192.168.88.0 mask 255.255.255.0</p>
</blockquote>
</blockquote>
<p>é›†ç¾¤åœ¨å±€åŸŸç½‘ä¸­ï¼Œä¸ä½¿ç”¨å…¶ä»–äº’è”ç½‘ä¸Šçš„æ—¶é—´</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<pre><code>#ä¿®æ”¹ /etc/ntpd.conf å†…å®¹
vim vim /etc/ntp.conf # 
</code></pre>
</blockquote>
</blockquote>
<pre><code>å°†21-24è¡Œå†…å®¹æ³¨é‡Šæ‰ï¼ˆæ³¨ï¼šåŸæ¥æœªæ³¨é‡Šï¼‰   
21 #server 0.centos.pool.ntp.org iburst 
22 #server 1.centos.pool.ntp.org iburst 
23 #server 2.centos.pool.ntp.org iburst 
24 #server 3.centos.pool.ntp.org iburst 
# åœ¨25è¡Œæ·»åŠ  server masterIP å³ä¸ºï¼š server 192.168.88.135
</code></pre>
<p>node å’Œ node3 ç›¸åŒæ“ä½œ<br>ä¸‰å°ä¸»æœºåŒæ—¶æ‰§è¡Œ</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   systemctl enable ntpd &amp;&amp; systemctl start ntpd</p>
</blockquote>
</blockquote>
<p>æŸ¥çœ‹ntpç«¯å£</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   [root@master etc]# ss -tupln | grep â€˜123â€™ udp UNCONN 0 0 192.168.88.135:123 <em>:</em> users:((â€œntpdâ€,pid&#x3D;54823,fd&#x3D;19)) udp UNCONN 0 0 127.0.0.1:123 <em>:</em> users:((â€œntpdâ€,pid&#x3D;54823,fd&#x3D;18)) udp UNCONN 0 0 <em>:123 <em>:</em> users:((â€œntpdâ€,pid&#x3D;54823,fd&#x3D;16)) udp UNCONN 0 0 [fe80::2832:5f98:5bc0:e621]%ens33:123 [::]:</em> users:((â€œntpdâ€,pid&#x3D;54823,fd&#x3D;23)) udp UNCONN 0 0 [::1]:123 [::]:* users:((â€œntpdâ€,pid&#x3D;54823,fd&#x3D;20)) udp UNCONN 0 0 [::]:123 [::]:* users:((â€œntpdâ€,pid&#x3D;54823,fd&#x3D;17))</p>
</blockquote>
</blockquote>
<p>é…ç½®å®Œæˆåä¸‰å°ä¸»æœºéƒ½éœ€è¦é‡å¯</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   shutdown -r 0</p>
</blockquote>
</blockquote>
<p>ä¸‰å°ä¸»æœºåŒæ—¶æ‰§è¡Œï¼ˆæ³¨ï¼šæ­¤è¿‡ç¨‹éœ€è¦5åˆ†é’Ÿå·¦å³ï¼‰</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   ntpstat</p>
</blockquote>
</blockquote>
<p>ä¸‰ã€sshå…å¯†é’¥ç™»é™†<br>SSHå…å¯†é’¥ç™»é™†å¯ä»¥æ›´åŠ æ–¹ä¾¿çš„å®ç°ä¸åŒè®¡ç®—æœºä¹‹é—´çš„è¿æ¥å’Œåˆ‡æ¢<br>master ç”Ÿæˆå…¬é’¥ç§é’¥ (ä¸€è·¯å›è½¦)<br>ssh-keygen </p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   #ç»“æœæ˜¾ç¤ºï¼š<br>    [root@master .ssh]# ssh-keygen Generating public&#x2F;private rsa key pair.<br>    Enter file in which to save the key (&#x2F;root&#x2F;.ssh&#x2F;id_rsa):<br>    Enter passphrase (empty for no passphrase):<br>    Enter same passphrase again:<br>    Your identification has been saved in &#x2F;root&#x2F;.ssh&#x2F;id_rsa.<br>    Your public key has been saved in &#x2F;root&#x2F;.ssh&#x2F;id_rsa.pub. T<br>    he key fingerprint is: SHA256:QUAgFH5KBc&#x2F;Erlf1JWSBbKeEepPJqMBqpWbc02&#x2F;uFj8 root@master The keyâ€™s randomart image is:<br>    +â€”[RSA 2048]â€”-+<br>    | .&#x3D;++oo+.o+.     |<br>    | . <em>. ..</em>.o .    |<br>    |. o.++ *.+ o     |</p>
</blockquote>
</blockquote>
<pre><code>|.o ++ B ...      | 
|o.=o.o .S        |
|.*oo.. .         | 
|+ .. . o         | 
| + E             | 
| =o .            | 
+----[SHA256]-----+
</code></pre>
<p>æŸ¥çœ‹éšè—çš„ .ssh æ–‡ä»¶</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   la -al .ssh </p>
</blockquote>
</blockquote>
<pre><code># ç»“æœæ˜¾ç¤º 
[root@master ~]# ls -al .ssh/ 
æ€»ç”¨é‡ 16 
drwx------ 2 root root 80 3æœˆ 10 21:52 . 
dr-xr-x---. 4 root root 175 3æœˆ 10 21:45 .. 
-rw------- 1 root root 393 3æœˆ 10 21:52 authorized_keys 
-rw------- 1 root root 1675 3æœˆ 10 21:48 id_rsa 
-rw-r--r-- 1 root root 393 3æœˆ 10 21:48 id_rsa.pub 
-rw-r--r-- 1 root root 366 3æœˆ 10 21:54 known_hosts
</code></pre>
<p>master é…ç½®å…å¯†ç™»å½•åˆ°master slave1 slave2</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   ssh-copy-id master<br>    ssh-copy-id slave1<br>    ssh-copy-id slave2</p>
</blockquote>
</blockquote>
<p>å››ã€å®‰è£…é…ç½® jdk<br>ç¼–è¯‘ç¯å¢ƒè½¯ä»¶å®‰è£…ç›®å½•</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   mkdir -p &#x2F;export&#x2F;server</p>
</blockquote>
</blockquote>
<p>JDK 1.8å®‰è£… ä¸Šä¼  jdk-8u241-linux-x64.tar.gzåˆ°&#x2F;export&#x2F;server&#x2F;ç›®å½•ä¸‹ å¹¶è§£å‹æ–‡ä»¶</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   tar -zxvf jdk-8u241-linux-x64.tar.gz</p>
</blockquote>
</blockquote>
<p>é…ç½®ç¯å¢ƒå˜é‡</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   vim &#x2F;etc&#x2F;profile export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar</p>
</blockquote>
</blockquote>
<p>é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   source &#x2F;etc&#x2F;profile</p>
</blockquote>
</blockquote>
<p>æŸ¥çœ‹ java ç‰ˆæœ¬å·</p>
<blockquote>
<blockquote>
<blockquote>
<p>java -version ç»“æœæ˜¾ç¤ºï¼š [root@master jdk1.8.0_241]# java -version java version â€œ1.8.0_241â€ Java(TM) SE Runtime Environment (build 1.8.0_241-b07) Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode)</p>
</blockquote>
</blockquote>
</blockquote>
<p>master èŠ‚ç‚¹å°† java ä¼ è¾“åˆ° slave1 å’Œ slave2</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   scp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@slave1:&#x2F;export&#x2F;server&#x2F; scp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@slave2:&#x2F;export&#x2F;server&#x2F;</p>
</blockquote>
</blockquote>
<p>é…ç½® slave1 å’Œ slave2 çš„ jdk ç¯å¢ƒå˜é‡ï¼ˆæ³¨ï¼šå’Œä¸Šæ–¹ master çš„é…ç½®æ–¹æ³•ä¸€æ ·ï¼‰<br>åœ¨ master slave1 å’Œslave2 åˆ›å»ºè½¯è¿æ¥</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server<br>    ln -s jdk1.8.0_241&#x2F; jdk</p>
</blockquote>
</blockquote>
<p>é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   source &#x2F;etc&#x2F;profile</p>
</blockquote>
</blockquote>
<p>zookeeperå®‰è£…é…ç½®<br>é…ç½®ä¸»æœºåå’ŒIPçš„æ˜ å°„å…³ç³»ï¼Œä¿®æ”¹ &#x2F;etc&#x2F;hosts æ–‡ä»¶ï¼Œæ·»åŠ  master.root slave1.root slave2.root</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<pre><code>vim /etc/hosts 
#ç»“æœæ˜¾ç¤º 
127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 
::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 
</code></pre>
</blockquote>
</blockquote>
<pre><code>192.168.88.135 master master.root 192.168.88.136 slave1 slave1.root 192.168.88.137 slave2 slave2.root
</code></pre>
<p>zookeeperå®‰è£… ä¸Šä¼  zookeeper-3.4.10.tar.gzåˆ°&#x2F;export&#x2F;server&#x2F;ç›®å½•ä¸‹ å¹¶è§£å‹æ–‡ä»¶</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server&#x2F; tar -zxvf zookeeper-3.4.10.tar.gz</p>
</blockquote>
</blockquote>
<p>åœ¨ &#x2F;export&#x2F;server ç›®å½•ä¸‹åˆ›å»ºè½¯è¿æ¥</p>
<blockquote>
<blockquote>
<blockquote>
<p>cd &#x2F;export&#x2F;server ln -s zookeeper-3.4.10&#x2F; zookeeper</p>
</blockquote>
</blockquote>
</blockquote>
<p>è¿›å…¥ &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; å°† zoo_sample.cfg æ–‡ä»¶å¤åˆ¶ä¸ºæ–°æ–‡ä»¶ zoo.cfg</p>
<blockquote>
<blockquote>
<blockquote>
<p>cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; cp zoo_sample.cfg zoo.cfg</p>
</blockquote>
</blockquote>
</blockquote>
<p>æ¥ä¸Šæ­¥ç»™ zoo.cfg æ·»åŠ å†…å®¹</p>
<blockquote>
<blockquote>
<blockquote>
<p>#Zookeeperçš„æ•°æ®å­˜æ”¾ç›®å½• dataDir&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas<br>    # ä¿ç•™å¤šå°‘ä¸ªå¿«ç…§ autopurge.snapRetainCount&#x3D;3<br>    # æ—¥å¿—å¤šå°‘å°æ—¶æ¸…ç†ä¸€æ¬¡ autopurge.purgeInterval&#x3D;1<br>    # é›†ç¾¤ä¸­æœåŠ¡å™¨åœ°å€ server.1&#x3D;master:2888:3888 server.2&#x3D;slave1:2888:3888 server.3&#x3D;slave2:2888:3888</p>
</blockquote>
</blockquote>
</blockquote>
<p>è¿›å…¥ &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas ç›®å½•åœ¨æ­¤ç›®å½•ä¸‹åˆ›å»º myid æ–‡ä»¶ï¼Œå°† 1 å†™å…¥è¿›å»</p>
<blockquote>
<blockquote>
<blockquote>
<p> cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdata touch myid echo â€˜1â€™ &gt; myid</p>
</blockquote>
</blockquote>
</blockquote>
<p>å°† master èŠ‚ç‚¹ä¸­ &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10 è·¯å¾„ä¸‹å†…å®¹æ¨é€ç»™slave1 å’Œ slave2</p>
<blockquote>
<blockquote>
<blockquote>
<p>scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F; slave1:$PWD scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F; slave2:$PWD</p>
</blockquote>
</blockquote>
</blockquote>
<p>æ¨é€æˆåŠŸåï¼Œåˆ†åˆ«åœ¨ slave1 å’Œ slave2 ä¸Šåˆ›å»ºè½¯è¿æ¥</p>
<blockquote>
<blockquote>
<blockquote>
<p>ln -s zookeeper-3.4.10&#x2F; zookeeper</p>
</blockquote>
</blockquote>
</blockquote>
<p>æ¥ä¸Šæ­¥æ¨é€å®Œæˆåå°† slave1 å’Œ slave2 çš„ &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F; æ–‡ä»¶å¤¹ä¸‹çš„ myidä¸­çš„å†…å®¹åˆ†åˆ«æ”¹ä¸º 2 å’Œ3</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F; ç»“æœæ˜¾ç¤ºï¼š [root@slave1 zkdatas]# vim myid [root@slave1 zkdatas]# more myid 2[root@slave2 zkdatas]# vim myid [root@slave2 zkdatas]# more myid 3</p>
</blockquote>
</blockquote>
<p>é…ç½®zookeeperçš„ç¯å¢ƒå˜é‡ï¼ˆæ³¨ï¼šä¸‰å°ä¸»æœºéƒ½éœ€è¦é…ç½®ï¼‰</p>
<blockquote>
<blockquote>
<blockquote>
<p> vim &#x2F;etc&#x2F;profile # zookeeper ç¯å¢ƒå˜é‡ export ZOOKEEPER_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper export PATH&#x3D;$PATH:$ZOOKEEPER_HOME&#x2F;bin</p>
</blockquote>
</blockquote>
</blockquote>
<p>é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶</p>
<blockquote>
<blockquote>
<blockquote>
<p>source &#x2F;etc&#x2F;profile</p>
</blockquote>
</blockquote>
</blockquote>
<p>è¿›å…¥ &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F;bin ç›®å½•ä¸‹å¯åŠ¨ zkServer.sh è„šæœ¬ ï¼ˆæ³¨ï¼šä¸‰å°éƒ½éœ€è¦åšï¼‰</p>
<blockquote>
<blockquote>
<blockquote>
<p>cd &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F;bin zkServer.sh start<br>    ç»“æœæ˜¾ç¤ºï¼š [root@master bin]# .&#x2F;zkServer.sh start ZooKeeper JMX enabled by default Using config: &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg Starting zookeeper â€¦ STARTED</p>
</blockquote>
</blockquote>
</blockquote>
<p>zookeeper çš„çŠ¶æ€</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<pre><code>zkServer.sh status ç»“æœæ˜¾ç¤ºï¼š [root@master server]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@slave1 server]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@slave2 conf]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader
</code></pre>
</blockquote>
</blockquote>
<p>jps ç»“æœæ˜¾ç¤ºï¼š </p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   [root@master server]# jps 125348 QuorumPeerMain 16311 Jps [root@slave1 server]# jps 126688 QuorumPeerMain 17685 Jps [root@slave2 conf]# jps 126733 QuorumPeerMain 17727 Jps</p>
</blockquote>
</blockquote>
<p>è„šæœ¬ä¸€é”®å¯åŠ¨</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   vim zkServer.sh<br>  #!&#x2F;bin&#x2F;bash<br>  if [ $# -eq 0 ] ;<br>  then<br>       echo â€œplease input param:start stopâ€<br>else<br>if [ $1 &#x3D; start ] ;then<br>   echo â€œ${1}ing masterâ€<br>   ssh master â€œsource &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh startâ€<br>         for i in {1..2}<br>          do<br>             echo â€œ${1}ping slave${i}â€ </p>
</blockquote>
</blockquote>
<pre><code>         ssh slave$&#123;i&#125; &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start&quot; 
    done 
</code></pre>
<p>fi<br>if [ $1 &#x3D; stop ];then<br>    echo â€œ${1}ping master â€œ<br>    ssh master â€œsource &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh stopâ€<br>    for i in {1..2}<br>    do<br>       echo â€œ${1}ping slave${i}â€ ssh slave${i} â€œsource &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh stopâ€<br>    done<br>fi<br>if [ $1 &#x3D; status ];then<br>    echo â€œ${1}ing masterâ€<br>    ssh master â€œsource &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh statusâ€<br>    for i in {1..2}<br>    do<br>       echo â€œ${1}ping slave${i}â€<br>        ssh slave${i} â€œsource &#x2F;etc&#x2F;profile;<br>&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh statusâ€<br>   done<br> fi<br> fi<br> #å°†æ–‡ä»¶æ”¾åœ¨ &#x2F;bin ç›®å½•ä¸‹ chmod +x zkServer-all.sh &amp;&amp; zkServer-all.sh</p>
<p>Hadoop å®‰è£…é…ç½®<br>æŠŠ hadoop-3.3.0-Centos7-64-with-snappy.tar.gz ä¸Šä¼ åˆ° &#x2F;export&#x2F;server å¹¶è§£å‹æ–‡ä»¶</p>
<blockquote>
<blockquote>
<blockquote>
<p>tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz</p>
</blockquote>
</blockquote>
</blockquote>
<pre><code>ä¿®æ”¹é…ç½®æ–‡ä»¶(è¿›å…¥è·¯å¾„ /export/server/hadoop-3.3.0/etc/hadoop)
cd /export/server/hadoop-3.3.0/etc/hadoop
hadoop-env.sh
#æ–‡ä»¶æœ€åæ·»åŠ  export JAVA_HOME=/export/server/jdk1.8.0_241 export HDFS_NAMENODE_USER=root export HDFS_DATANODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root export YARN_RESOURCEMANAGER_USER=root export YARN_NODEMANAGER_USER=root
core-site.xml
&lt;!-- è®¾ç½®é»˜è®¤ä½¿ç”¨çš„æ–‡ä»¶ç³»ç»Ÿ Hadoopæ”¯æŒfileã€HDFSã€GFSã€ali|Amazonäº‘ç­‰æ–‡ä»¶ç³»ç»Ÿ - -&gt;
&lt;property&gt; 
         &lt;name&gt;fs.defaultFS&lt;/name&gt; 
         &lt;value&gt;hdfs://master:8020&lt;/value&gt; 
         &lt;/property&gt; 
&lt;!-- è®¾ç½®Hadoopæœ¬åœ°ä¿å­˜æ•°æ®è·¯å¾„ --&gt; 
&lt;property&gt; 
    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; 
    &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt; 
&lt;/property&gt; 
&lt;!-- è®¾ç½®HDFS web UIç”¨æˆ·èº«ä»½ --&gt; 
&lt;property&gt; 
     &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; 
     &lt;value&gt;root&lt;/value&gt; 
&lt;/property&gt; 
</code></pre>
<!-- æ•´åˆhive ç”¨æˆ·ä»£ç†è®¾ç½® -->
<property> 
    <name>hadoop.proxyuser.root.hosts</name> 
    <value>*</value> 
    </property>
     <property> 
     <name>hadoop.proxyuser.root.groups</name> 
     <value>*</value> 
     </property> <
     !-- æ–‡ä»¶ç³»ç»Ÿåƒåœ¾æ¡¶ä¿å­˜æ—¶é—´ --> <property> 
     <name>fs.trash.interval</name>
      <value>1440</value> 
    </property>
    hdfs-site.xml
    <!-- è®¾ç½®SNNè¿›ç¨‹è¿è¡Œæœºå™¨ä½ç½®ä¿¡æ¯ --> 
    <property> 
    <name>dfs.namenode.secondary.http-address</name> <value>slave1:9868</value>
     </property>
    mapred-site.xml
    <!-- è®¾ç½®MRç¨‹åºé»˜è®¤è¿è¡Œæ¨¡å¼ï¼š yarné›†ç¾¤æ¨¡å¼ localæœ¬åœ°æ¨¡å¼ --> <property> 
    <name>mapreduce.framework.name</name> 
    <value>yarn</value> 
    </property> 
    <!-- MRç¨‹åºå†å²æœåŠ¡åœ°å€ -->
     <property> 
    <name>mapreduce.jobhistory.address</name> 
    <value>master:10020</value> 
    </property> 
    <!-- MRç¨‹åºå†å²æœåŠ¡å™¨webç«¯åœ°å€ --> 
    <property> 
    <name>mapreduce.jobhistory.webapp.address</name> <value>master:19888</value> 
    </property> 
    <property> 
    <name>yarn.app.mapreduce.am.env</name> 
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value> 
    </property> 
    <property> 
    <name>mapreduce.map.env</name> 
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value> 
    </property> 
    <property> 
    <name>mapreduce.reduce.env</name>
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value> 
    </property>
yarn-site.xml
<!-- è®¾ç½®YARNé›†ç¾¤ä¸»è§’è‰²è¿è¡Œæœºå™¨ä½ç½® --> 
<property> 
<name>yarn.resourcemanager.hostname</name> 
<value>master</value> 
</property> 
<property>
 <name>yarn.nodemanager.aux-services</name> 
 <value>mapreduce_shuffle</value> 
 </property>
  <!-- æ˜¯å¦å°†å¯¹å®¹å™¨å®æ–½ç‰©ç†å†…å­˜é™åˆ¶ --> 
  <property>
   <name>yarn.nodemanager.pmem-check-enabled</name> 
   <value>false</value>
    </property>
     <!-- æ˜¯å¦å°†å¯¹å®¹å™¨å®æ–½è™šæ‹Ÿå†…å­˜é™åˆ¶ã€‚ --> 
     <property> 
     <name>yarn.nodemanager.vmem-check-enabled</name>
      <value>false</value> 
      </property>
       <!-- å¼€å¯æ—¥å¿—èšé›† --> 
       <property> <name>yarn.log-aggregation-enable</name> <value>true</value> 
       </property>
        <!-- è®¾ç½®yarnå†å²æœåŠ¡å™¨åœ°å€ --> 
        <property> 
        <name>yarn.log.server.url</name> 
        <value>http://master:19888/jobhistory/logs</value> 
        </property> 
        <!-- å†å²æ—¥å¿—ä¿å­˜çš„æ—¶é—´ 7å¤© --> 
        <property> 
        <name>yarn.log-aggregation.retain-seconds</name> <value>604800</value>
         </property>

<pre><code>     node1
    node2
      node3
</code></pre>
<p>åˆ†å‘åŒæ­¥hadoopå®‰è£…åŒ…</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server<br>     scp -r hadoop-3.3.0 root@slave1:$PWD<br>     scp -r hadoop-3.3.0 root@slave2:$PWD<br>å°†hadoopæ·»åŠ åˆ°ç¯å¢ƒå˜é‡</p>
<blockquote>
</blockquote>
<p>   vim &#x2F;etc&#x2F;profile export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin</p>
</blockquote>
</blockquote>
<p>é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   source &#x2F;etc&#x2F;profile</p>
</blockquote>
</blockquote>
<pre><code>Hadoopé›†ç¾¤å¯åŠ¨
æ ¼å¼åŒ–namenodeï¼ˆåªæœ‰é¦–æ¬¡å¯åŠ¨éœ€è¦æ ¼å¼åŒ–ï¼‰
hdfs namenode -format
</code></pre>
<p>è„šæœ¬ä¸€é”®å¯åŠ¨<br>    [root@master ~]# start-dfs.sh Starting namenodes on [master] ä¸Šä¸€æ¬¡ç™»å½•ï¼šäº” 3æœˆ 11 21:27:24 CST 2022pts&#x2F;0 ä¸Š Starting datanodes ä¸Šä¸€æ¬¡ç™»å½•ï¼šäº” 3æœˆ 11 21:27:32 CST 2022pts&#x2F;0 ä¸Š Starting secondary namenodes [slave1] ä¸Šä¸€æ¬¡ç™»å½•ï¼šäº” 3æœˆ 11 21:27:35 CST 2022pts&#x2F;0 ä¸Š </p>
<pre><code>[root@master ~]# start-yarn.sh Starting resourcemanager ä¸Šä¸€æ¬¡ç™»å½•ï¼šäº” 3æœˆ 11 21:27:41 CST 2022pts/0 ä¸Š Starting nodemanagers ä¸Šä¸€æ¬¡ç™»å½•ï¼šäº” 3æœˆ 11 21:27:51 CST 2022pts/0 ä¸Š
å¯åŠ¨å è¾“å…¥ jps æŸ¥çœ‹

[root@master ~]# jps 127729 NameNode 127937 DataNode 14105 Jps 128812 NodeManager 128591 ResourceManager [root@slave1 hadoop]# jps 121889 NodeManager 121559 SecondaryNameNode 7014 Jps 121369 DataNode
 [root@slave2 hadoop]# jps
 6673 Jps 121543 NodeManager 121098 DataNode
</code></pre>
<p>WEBé¡µé¢<br>HDFSé›†ç¾¤ï¼š</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<pre><code>http://master:9870/
</code></pre>
</blockquote>
</blockquote>
<p>YARNé›†ç¾¤ï¼š</p>
<blockquote>
<blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://master:9870/">http://master:9870/</a></p>
</blockquote>
</blockquote>
</blockquote>
</article><article><h1 id="ã€ŠSpark-HA-amp-Yarné…ç½®ã€‹"><a href="#ã€ŠSpark-HA-amp-Yarné…ç½®ã€‹" class="headerlink" title="ã€ŠSpark HA &amp; Yarné…ç½®ã€‹"></a>ã€ŠSpark HA &amp; Yarné…ç½®ã€‹</h1><p>Spark-Standalone-HAæ¨¡å¼<br>Spark Standaloneé›†ç¾¤æ˜¯Master-Slavesæ¶æ„çš„é›†ç¾¤æ¨¡å¼,å’Œå¤§éƒ¨åˆ†çš„Master-Slavesç»“æ„é›†ç¾¤ä¸€æ ·,å­˜åœ¨<br>ç€Master å•ç‚¹æ•…éšœ(SPOF)çš„é—®é¢˜ã€‚ç®€å•ç†è§£ä¸ºï¼Œspark-Standalone æ¨¡å¼ä¸‹ä¸º master èŠ‚ç‚¹æ§åˆ¶å…¶ä»–èŠ‚<br>ç‚¹ï¼Œå½“ master èŠ‚ç‚¹å‡ºç°æ•…éšœæ—¶ï¼Œé›†ç¾¤å°±ä¸å¯ç”¨äº†ã€‚ spark-Standalone-HA æ¨¡å¼ä¸‹<br>master èŠ‚ç‚¹ä¸å›ºå®šï¼Œå½“ä¸€ä¸ªå®•æœºæ—¶ï¼Œç«‹å³æ¢å¦ä¸€å°ä¸º master ä¿éšœä¸å‡ºç°æ•…éšœã€‚<br>æ­¤å¤„å› ä¸ºå…ˆå‰é…ç½®æ—¶çš„ zookeeper ç‰ˆæœ¬å’Œ spark ç‰ˆæœ¬ä¸å¤ªå…¼å®¹ï¼Œå¯¼è‡´æ­¤æ¨¡å¼æœ‰æ•…éšœï¼Œéœ€è¦é‡æ–°ä¸‹<br>è½½é…ç½®æ–°çš„ç‰ˆæœ¬çš„ zookeeper<br>é…ç½®ä¹‹å‰éœ€è¦åˆ é™¤ä¸‰å°ä¸»æœºçš„ æ—§ç‰ˆ zookeeper ä»¥åŠ å¯¹åº”çš„è½¯è¿æ¥<br>åœ¨ master èŠ‚ç‚¹ä¸Šé‡æ–°è¿›è¡Œå‰é¢é…ç½®çš„ zookeeper æ“ä½œ</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   1.ä¸Šä¼ apache-zookeeper-3.7.0-bin.tar.gz åˆ°&#x2F;export&#x2F;server&#x2F;ç›®å½•ä¸‹ å¹¶è§£å‹æ–‡ä»¶ 2.åœ¨ &#x2F;export&#x2F;server ç›®å½•ä¸‹åˆ›å»ºè½¯è¿æ¥ 3.è¿›å…¥ &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; å°† zoo_sample.cfg æ–‡ä»¶å¤åˆ¶ä¸ºæ–°æ–‡ä»¶ zoo.cfg 4.æ¥ä¸Šæ­¥ç»™ zoo.cfg æ·»åŠ å†…å®¹ 5.è¿›å…¥ &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas ç›®å½•åœ¨æ­¤ç›®å½•ä¸‹åˆ›å»º myid æ–‡ä»¶ï¼Œå°† 1 å†™å…¥è¿› å»6.å°† master èŠ‚ç‚¹ä¸­ &#x2F;export&#x2F;server&#x2F;zookeeper-3.7.0 è·¯å¾„ä¸‹å†…å®¹æ¨é€ç»™slave1 å’Œ slave2 7.æ¨é€æˆåŠŸåï¼Œåˆ†åˆ«åœ¨ slave1 å’Œ slave2 ä¸Šåˆ›å»ºè½¯è¿æ¥ 8.æ¥ä¸Šæ­¥æ¨é€å®Œæˆåå°† slave1 å’Œ slave2 çš„ &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;æ–‡ä»¶å¤¹ ä¸‹çš„ myid ä¸­çš„å†…å®¹åˆ†åˆ«æ”¹ä¸º 2 å’Œ 3 é…ç½®ç¯å¢ƒå˜é‡ï¼š å› å…ˆå‰é…ç½® zookeeper æ—¶å€™åˆ›å»ºè¿‡è½¯è¿æ¥ä¸”ä»¥ â€™zookeeperâ€˜ ä¸ºè·¯å¾„ï¼Œæ‰€ä»¥ä¸ç”¨é…ç½®ç¯å¢ƒå˜é‡ï¼Œæ­¤ å¤„ä¹Ÿæ˜¯åˆ›å»ºè½¯è¿æ¥çš„æ–¹ä¾¿ä¹‹å¤„</p>
</blockquote>
</blockquote>
<p>è¿›å…¥ &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf æ–‡ä»¶å¤¹ ä¿®æ”¹ spark-env.sh æ–‡ä»¶å†…å®¹</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf<br>    vim spark-env.sh</p>
</blockquote>
</blockquote>
<p>ä¸º 83 è¡Œå†…å®¹åŠ ä¸Šæ³¨é‡Šï¼Œæ­¤éƒ¨åˆ†åŸä¸ºæŒ‡å®š æŸå°ä¸»æœº åš master ï¼ŒåŠ ä¸Šæ³¨é‡Šåå³ä¸º ä»»ä½•ä¸»æœºéƒ½<br>å¯ä»¥åš master</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   ç»“æœæ˜¾ç¤ºï¼š<br>     â€¦â€¦<br>      82 # å‘ŠçŸ¥Sparkçš„masterè¿è¡Œåœ¨å“ªä¸ªæœºå™¨ä¸Š 83 # export SPARK_MASTER_HOST&#x3D;master<br>     â€¦â€¦â€¦</p>
</blockquote>
</blockquote>
<p>æ–‡æœ«æ·»åŠ å†…å®¹</p>
<blockquote>
<blockquote>
<blockquote>
<p>SPARK_DAEMON_JAVA_OPTS&#x3D;â€-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER -<br>       Dspark.deploy.zookeeper.url&#x3D;master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark-haâ€<br>        #spark.deploy.recoveryMode<br>        æŒ‡å®šHAæ¨¡å¼ åŸºäºZookeeperå®ç°<br>        #æŒ‡å®šZookeeperçš„è¿æ¥åœ°å€<br>        #æŒ‡å®šåœ¨Zookeeperä¸­æ³¨å†Œä¸´æ—¶èŠ‚ç‚¹çš„è·¯å¾„</p>
</blockquote>
</blockquote>
</blockquote>
<p>åˆ†å‘ spark-env.sh åˆ° salve1 å’Œ slave2 ä¸Š</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<pre><code>scp spark-env.sh slave1:/export/server/spark/conf/ 
scp spark-env.sh slave2:/export/server/spark/conf/
</code></pre>
</blockquote>
</blockquote>
<p>å¯åŠ¨ä¹‹å‰ç¡®ä¿ Zookeeper å’Œ HDFS å‡å·²ç»å¯åŠ¨<br>å¯åŠ¨é›†ç¾¤:</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   #åœ¨ master ä¸Š å¯åŠ¨ä¸€ä¸ªmaster å’Œå…¨éƒ¨worker &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-all.sh # æ³¨æ„, ä¸‹é¢å‘½ä»¤åœ¨ slave1 ä¸Šæ‰§è¡Œ å¯åŠ¨ slave1 ä¸Šçš„ master åšå¤‡ç”¨ master &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-master.sh<br>    ç»“æœæ˜¾ç¤ºï¼š<br>    (base) [root@master ~]# jps<br>    37328 DataNode<br>    41589 Master<br>    35798 QuorumPeerMain<br>    38521 ResourceManager<br>    46281 Jps<br>    38907 NodeManager<br>    41821 Worker<br>    36958 NameNode (base)<br>    [root@slave1 sbin]# jps<br>    36631 DataNode<br>    48135 Master<br>    35385 QuorumPeerMain<br>    37961 NodeManager<br>    40970 Worker<br>    48282 Jps<br>    37276 SecondaryNameNode</p>
</blockquote>
</blockquote>
<p>è®¿é—® WebUI ç•Œé¢</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<pre><code>http://master:8081/
</code></pre>
<p>   <a target="_blank" rel="noopener" href="http://slave1:8082/">http://slave1:8082/</a></p>
</blockquote>
</blockquote>
<p>æ­¤æ—¶ kill æ‰ master ä¸Šçš„ master å‡è®¾ master ä¸»æœºå®•æœºæ‰</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   #masterä¸»æœº master çš„è¿›ç¨‹å· kill -9 41589 ç»“æœæ˜¾ç¤ºï¼š (base) [root@master ~]# jps 37328 DataNode 90336 Jps 35798 QuorumPeerMain 38521 ResourceManager 38907 NodeManager 41821 Worker 36958 NameNode</p>
</blockquote>
</blockquote>
<p>è®¿é—® slave1 çš„ WebUI</p>
<blockquote>
<blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://slave1:8082/">http://slave1:8082/</a></p>
</blockquote>
</blockquote>
</blockquote>
<p>è¿›è¡Œä¸»å¤‡åˆ‡æ¢çš„æµ‹è¯•<br>æäº¤ä¸€ä¸ª spark ä»»åŠ¡åˆ°å½“å‰ æ´»è·ƒçš„ masterä¸Š :</p>
<blockquote>
<blockquote>
<blockquote>
<p>&#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit â€“master spark:&#x2F;&#x2F;master:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000</p>
</blockquote>
</blockquote>
</blockquote>
<p>å¤åˆ¶æ ‡ç­¾ kill æ‰ master çš„ è¿›ç¨‹å·<br>å†æ¬¡è®¿é—® master çš„ WebUI</p>
<blockquote>
<blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://master:8081/">http://master:8081/</a><br>      ç½‘é¡µè®¿é—®ä¸äº†ï¼</p>
</blockquote>
</blockquote>
</blockquote>
<p>å†æ¬¡è®¿é—® slave1 çš„ WebUI</p>
<blockquote>
<blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://slave1:8082/">http://slave1:8082/</a></p>
</blockquote>
</blockquote>
</blockquote>
<p>å¯ä»¥çœ‹åˆ°å½“å‰æ´»è·ƒçš„ master æç¤ºä¿¡æ¯</p>
<blockquote>
<blockquote>
<blockquote>
<p>(base) [root@master ~]# &#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit â€“master spark:&#x2F;&#x2F;master:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000 22&#x2F;03&#x2F;29 16:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platformâ€¦ using builtin-java classes where applicable 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnectâ€¦ 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnectionâ€¦ 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnectâ€¦ Pi is roughly 3.140960 (base) [root@master ~]#</p>
</blockquote>
</blockquote>
</blockquote>
<p>Spark On YARNæ¨¡å¼</p>
<blockquote>
<blockquote>
<blockquote>
<p>åœ¨å·²æœ‰YARNé›†ç¾¤çš„å‰æä¸‹åœ¨å•ç‹¬å‡†å¤‡Spark StandAloneé›†ç¾¤,å¯¹èµ„æºçš„åˆ©ç”¨å°±ä¸é«˜.Spark On YARN, æ— </p>
</blockquote>
</blockquote>
</blockquote>
<p>éœ€éƒ¨ç½²Sparké›†ç¾¤, åªè¦æ‰¾ä¸€å°æœåŠ¡å™¨, å……å½“Sparkçš„å®¢æˆ·ç«¯<br>ä¿è¯ HADOOP_CONF_å’Œ DIR_YARN_CONF_DIR å·²ç»é…ç½®åœ¨ spark-env.sh å’Œç¯å¢ƒå˜é‡ä¸­ ï¼ˆæ³¨: å‰é¢é…ç½®spark-Standlone æ—¶å·²ç»é…ç½®è¿‡æ­¤é¡¹äº†ï¼‰</p>
<blockquote>
<blockquote>
<blockquote>
<p>spark-env.sh æ–‡ä»¶éƒ¨åˆ†æ˜¾ç¤ºï¼š â€¦. 77 ## HADOOPè½¯ä»¶é…ç½®æ–‡ä»¶ç›®å½•ï¼Œè¯»å–HDFSä¸Šæ–‡ä»¶å’Œè¿è¡ŒYARNé›†ç¾¤ 78 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop 79 YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop â€¦.</p>
</blockquote>
</blockquote>
</blockquote>
<p>é“¾æ¥åˆ° YARN ä¸­ï¼ˆæ³¨: äº¤äº’å¼ç¯å¢ƒ pyspark å’Œ spark-shell æ— æ³•è¿è¡Œ clusteræ¨¡å¼ï¼‰<br>bin&#x2F;pyspark â€“master yarn â€“deploy-mode client|cluster # â€“deploy-mode é€‰é¡¹æ˜¯æŒ‡å®šéƒ¨ç½²æ¨¡å¼, é»˜è®¤æ˜¯ å®¢æˆ·ç«¯æ¨¡å¼ # clientå°±æ˜¯å®¢æˆ·ç«¯æ¨¡å¼ # clusterå°±æ˜¯é›†ç¾¤æ¨¡å¼ # â€“deploy-mode ä»…å¯ä»¥ç”¨åœ¨YARNæ¨¡å¼ä¸‹<br> bin&#x2F;spark-shell â€“master yarn â€“deploy-mode client|cluster<br>bin&#x2F;spark-submit â€“master yarn â€“deploy-mode client|cluster &#x2F;xxx&#x2F;xxx&#x2F;xxx.py å‚æ•°</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>  spark-submit å’Œ spark-shell å’Œ pysparkçš„ç›¸å…³å‚æ•°</p>
</blockquote>
</blockquote>
<ul>
<li>bin&#x2F;pyspark: pysparkè§£é‡Šå™¨sparkç¯å¢ƒ - bin&#x2F;spark-shell: scalaè§£é‡Šå™¨sparkç¯å¢ƒ - bin&#x2F;spark-submit: æäº¤jaråŒ…æˆ–Pythonæ–‡ä»¶æ‰§è¡Œçš„å·¥å…· - bin&#x2F;spark-sql: sparksqlå®¢æˆ·ç«¯å·¥å…·<br>  è¿™4ä¸ªå®¢æˆ·ç«¯å·¥å…·çš„å‚æ•°åŸºæœ¬é€šç”¨.ä»¥spark-submit ä¸ºä¾‹: bin&#x2F;spark-submit â€“master spark:&#x2F;&#x2F;master:7077 xxx.py&#96;<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>  Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]<br>   Usage: spark-submit â€“kill [submission ID] â€“master [spark:&#x2F;&#x2F;â€¦]<br>   Usage: spark-submit â€“status [submission ID] â€“master [spark:&#x2F;&#x2F;â€¦]<br>   Usage: spark-submit run-example [options] example-class [example args] </p>
<blockquote>
</blockquote>
<p>   Options: â€“master MASTER_URL spark:&#x2F;&#x2F;host:port, mesos:&#x2F;&#x2F;host:port, yarn, k8s:&#x2F;&#x2F;<a href="https://host:port">https://host:port</a>, or </p>
<blockquote>
</blockquote>
<p>   local (Default: local[*]). â€“deploy-mode DEPLOY_MODE éƒ¨ç½²æ¨¡å¼ client æˆ–è€… cluster é»˜è®¤æ˜¯client â€“class CLASS_NAME è¿è¡Œjavaæˆ–è€…scala class(for Java &#x2F; Scala apps). â€“name NAME ç¨‹åºçš„åå­— â€“jars JARS Comma-separated list of jars to include on the </p>
<blockquote>
</blockquote>
<p>   driver and executor classpaths. â€“packages Comma-separated list of maven coordinates of </p>
<blockquote>
</blockquote>
<p>   jars to include on the driver and executor classpaths. Will </p>
<blockquote>
</blockquote>
<p>   search the local maven repo, then maven central and any </p>
<blockquote>
</blockquote>
<p>   additional remote repositories given by â€“repositories. The </p>
<blockquote>
</blockquote>
<p>   format for the coordinates should be </p>
<blockquote>
</blockquote>
<p>   groupId:artifactId:version.<br>   â€“exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in<br>â€“ packages to avoid dependency conflicts. â€“repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with<br> â€“ packages.<br> â€“py-files PY_FILES æŒ‡å®šPythonç¨‹åºä¾èµ–çš„å…¶å®ƒpythonæ–‡ä»¶<br> â€“files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName).<br>  â€“archives ARCHIVES Comma-separated list of archives to be extracted into the working directory of each executor.<br> â€“conf,<br> -c PROP&#x3D;VALUE æ‰‹åŠ¨æŒ‡å®šé…ç½®<br> â€“properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf&#x2F;spark- defaults.conf. â€“driver-memory MEM Driverçš„å¯ç”¨å†…å­˜(Default: 1024M). â€“driver-java-options Driverçš„ä¸€äº›Javaé€‰é¡¹ â€“driver-library-path Extra library path entries to pass to the driver. â€“driver-class-path Extra class path entries to pass to the driver. Note that jars added with â€“jars are automatically included in the classpath.<br> â€“executor-memory MEM Executorçš„å†…å­˜ (Default: 1G).<br> â€“proxy-user NAME User to impersonate when submitting the application. This argument does not work with<br> â€“principal &#x2F;<br> â€“keytab.<br> â€“help,<br> -h æ˜¾ç¤ºå¸®åŠ©æ–‡ä»¶<br>  â€“verbose,<br>  -v Print additional debug output. â€“version, æ‰“å°ç‰ˆæœ¬ Cluster deploy mode only(é›†ç¾¤æ¨¡å¼ä¸“å±):<br>   â€“driver-cores NUM Driverå¯ç”¨çš„çš„CPUæ ¸æ•°(Default: 1). Spark standalone or Mesos with cluster deploy mode only:<br>   â€“supervise å¦‚æœç»™å®š, å¯ä»¥å°è¯•é‡å¯Driver Spark standalone, Mesos or K8s with cluster deploy mode only:<br>   â€“kill SUBMISSION_ID æŒ‡å®šç¨‹åºID kill â€“status SUBMISSION_ID æŒ‡å®šç¨‹åºID æŸ¥çœ‹è¿è¡ŒçŠ¶æ€ Spark standalone, Mesos and Kubernetes only:<br>   â€“total-executor-cores NUM æ•´ä¸ªä»»åŠ¡å¯ä»¥ç»™Executorå¤šå°‘ä¸ªCPUæ ¸å¿ƒç”¨ Spark standalone, YARN and Kubernetes only:<br>    â€“executor-cores NUM å•ä¸ªExecutorèƒ½ä½¿ç”¨å¤šå°‘CPUæ ¸å¿ƒ Spark on YARN and Kubernetes only(YARNæ¨¡å¼ä¸‹):<br>    â€“num-executors NUM Executoråº”è¯¥å¼€å¯å‡ ä¸ª<br>    â€“principal PRINCIPAL Principal to be used to login to KDC.<br>    â€“keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. Spark on YARN only:<br>    â€“queue QUEUE_NAME æŒ‡å®šè¿è¡Œçš„YARNé˜Ÿåˆ—(Default: â€œdefaultâ€)</p>
</blockquote>
</blockquote>
</li>
</ul>
<p>å¯åŠ¨ YARN çš„å†å²æœåŠ¡å™¨</p>
<blockquote>
<blockquote>
<blockquote>
<p>cd &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;sbin .&#x2F;mr-jobhistory-daemon.sh start historyserver</p>
</blockquote>
</blockquote>
</blockquote>
<p>è®¿é—®WebUIç•Œé¢</p>
<blockquote>
<blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://master:19888/">http://master:19888/</a></p>
</blockquote>
</blockquote>
</blockquote>
<p>client æ¨¡å¼æµ‹è¯•</p>
<blockquote>
<blockquote>
<blockquote>
<p>SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark ${SPARK_HOME}&#x2F;bin&#x2F;spark-submit â€“master yarn â€“deploy-mode client â€“ driver-memory 512m â€“executor-memory 512m â€“num-executors 1 â€“total- executor-cores 2 ${SPARK_HOME}&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py</p>
</blockquote>
</blockquote>
</blockquote>
<p>cluster æ¨¡å¼æµ‹è¯•</p>
<blockquote>
<blockquote>
<blockquote>
<p>SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark ${SPARK_HOME}&#x2F;bin&#x2F;spark-submit â€“master yarn â€“deploy-mode cluster â€“driver- memory 512m â€“executor-memory 512m â€“num-executors 1 â€“total-executor-cores 2 â€“conf â€œspark.pyspark.driver.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3â€ â€“conf â€œspark.pyspark.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3â€ ${SPARK_HOME}&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 3**</p>
</blockquote>
</blockquote>
</blockquote>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/#Publications"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/"> About</a></li><li class="nav_item"><a class="nav-page" target="_blank" rel="noopener" href="https://phower.me"> Blog</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 - 2022 by ğŸ˜Š</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>